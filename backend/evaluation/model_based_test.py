import sys
from pathlib import Path
import pandas as pd
import numpy as np
from functools import lru_cache

# ----------------- ê²½ë¡œ / ëª¨ë“ˆ ì„¤ì • -----------------
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))

# âœ… ì˜¤ì§ model_based ëª¨ë“ˆë§Œ ì‚¬ìš©
from backend.recommend import model_based

# ----------------- ì„¤ì • ê°’ -----------------
MODEL_PATH = PROJECT_ROOT / "backend" / "data" / "model" / "bpr_model.pt"
META_PATH  = PROJECT_ROOT / "backend" / "data" / "model" / "bpr_meta.pkl"

TEST_CSV = PROJECT_ROOT / "backend" / "processed" / "test_6cols.csv"

# í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ìƒ˜í”Œ ê°œìˆ˜ (None ì´ë©´ ì „ì²´)
SAMPLE_SIZE = 500
# SAMPLE_SIZE = None

# í•œ ìœ ì €ì— ëŒ€í•´ ì¶”ì²œë°›ì„ ê°œìˆ˜ (í‰ê°€ìš© Top-K)
K_EVAL = 20    # ì˜ˆ: Top-20 ì¶”ì²œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€


# ----------------- ëª¨ë¸ ë¡œë“œ -----------------
print(f"Loading BPR-MF model from:\n  {MODEL_PATH}\n  {META_PATH}")
model_based.load_model(MODEL_PATH, META_PATH)

# ----------------- í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ -----------------
if not TEST_CSV.exists():
    raise FileNotFoundError(f"Test CSV not found: {TEST_CSV}")

if SAMPLE_SIZE is None:
    print(f"Loading full test data from: {TEST_CSV}")
    df_test = pd.read_csv(TEST_CSV)
else:
    print(f"Loading first {SAMPLE_SIZE} rows from: {TEST_CSV}")
    df_test = pd.read_csv(TEST_CSV, nrows=SAMPLE_SIZE)

print(f"âœ… Test set loaded. Shape: {df_test.shape}")
print(df_test.head())


# ----------------- ì¶”ì²œ ê²°ê³¼ ìºì‹œ -----------------
@lru_cache(maxsize=100_000)
def get_recommended_titles(u_id: int, k: int = K_EVAL):
    """
    íŠ¹ì • user_idì— ëŒ€í•´ model_based.recommend_by_model ì„ í˜¸ì¶œí•˜ê³ ,
    ì¶”ì²œëœ title ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜.
    """
    try:
        res = model_based.recommend_by_model(u_id, n_recommendations=k)
    except Exception as e:
        print(f"âš ï¸ recommend_by_model raised for user {u_id}: {e}")
        return []

    if not isinstance(res, dict):
        return []

    result_list = res.get("result", [])
    if not isinstance(result_list, list) or len(result_list) == 0:
        return []

    titles = []
    for r in result_list:
        if not isinstance(r, dict):
            continue
        t = r.get("title")
        if t is None:
            continue
        titles.append(t)
    return titles


# ----------------- NDCG ê³„ì‚° í•¨ìˆ˜ -----------------
def dcg_at_k(relevances, k):
    """
    relevances: ê¸¸ì´ <= k ì¸ ë¦¬ìŠ¤íŠ¸, ê° ì›ì†ŒëŠ” 0 ë˜ëŠ” 1 (ë˜ëŠ” ì •ìˆ˜ ì ìˆ˜)
    DCG@k = Î£ (2^rel_i - 1) / log2(i+2)
    """
    relevances = np.asarray(relevances)[:k]
    if relevances.size == 0:
        return 0.0
    discounts = np.log2(np.arange(2, relevances.size + 2))
    gains = (2 ** relevances - 1) / discounts
    return float(gains.sum())


def ndcg_at_k(recommended, relevant_set, k):
    """
    recommended: ì¶”ì²œëœ title ë¦¬ìŠ¤íŠ¸
    relevant_set: ì‹¤ì œ ì •ë‹µ title ì§‘í•© (set)
    k: ìƒìœ„ ëª‡ ê°œê¹Œì§€ ë³¼ì§€
    """
    if len(recommended) == 0 or len(relevant_set) == 0:
        return 0.0

    # ì¶”ì²œ ìˆœì„œëŒ€ë¡œ relevance (0 ë˜ëŠ” 1) ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    rec_k = recommended[:k]
    rels = [1 if t in relevant_set else 0 for t in rec_k]

    dcg = dcg_at_k(rels, k)

    # ideal DCG: relevanceê°€ 1ì¸ ì•„ì´í…œë“¤ì„ ìƒìœ„ì— ëª°ì•„ë†¨ë‹¤ê³  ê°€ì •
    ideal_rels = [1] * min(len(relevant_set), k)
    idcg = dcg_at_k(ideal_rels, k)

    if idcg == 0:
        return 0.0
    return dcg / idcg


# ----------------- ìœ ì € ë‹¨ìœ„ Top-K í‰ê°€ -----------------
print("\nğŸ” Evaluating model-based recommendations with Top-K metrics ...")

user_groups = df_test.groupby("user_id")

num_users_total = 0              # testì— ë“±ì¥í•œ ìœ ì € ìˆ˜
num_users_with_pos = 0           # testì—ì„œ ì–‘ì„±(label=1)ì„ ê°€ì§„ ìœ ì € ìˆ˜
num_users_with_hit = 0           # ì ì–´ë„ í•˜ë‚˜ëŠ” ë§ì¶˜ ìœ ì € ìˆ˜ (Hit@K)

sum_precision = 0.0
sum_recall = 0.0
sum_ndcg = 0.0

for u, group in user_groups:
    num_users_total += 1
    # ì´ ìœ ì €ê°€ testì—ì„œ ì‹¤ì œë¡œ ì¢‹ì•„í•œ(ì–‘ì„±) íƒ€ì´í‹€ ì§‘í•©
    true_pos_titles = set(group.loc[group["is_recommended"] == 1, "title"])

    if len(true_pos_titles) == 0:
        # ì´ ìœ ì €ëŠ” testì—ì„œ ì–‘ì„± ìƒ˜í”Œì´ ì—†ìœ¼ë¯€ë¡œ í‰ê°€ì—ì„œ ì œì™¸
        continue

    num_users_with_pos += 1

    # ëª¨ë¸ì´ ì¶”ì²œí•œ Top-K íƒ€ì´í‹€ ë¦¬ìŠ¤íŠ¸
    rec_titles = get_recommended_titles(int(u), k=K_EVAL)

    if len(rec_titles) == 0:
        # ì¶”ì²œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´, precision/recall/ndcg ëª¨ë‘ 0
        continue

    rec_set = set(rec_titles)
    hit_items = true_pos_titles & rec_set   # êµì§‘í•©

    hits = len(hit_items)

    if hits > 0:
        num_users_with_hit += 1

    # Precision@K: Top-K ì¤‘ì—ì„œ ë§ì¶˜ ë¹„ìœ¨
    precision_k = hits / len(rec_titles) if len(rec_titles) > 0 else 0.0

    # Recall@K: ì‹¤ì œ ì •ë‹µ ì¤‘ì—ì„œ Top-Kì— ë“¤ì–´ê°„ ë¹„ìœ¨
    recall_k = hits / len(true_pos_titles) if len(true_pos_titles) > 0 else 0.0

    # NDCG@K: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì§€í‘œ
    ndcg_k = ndcg_at_k(rec_titles, true_pos_titles, K_EVAL)

    sum_precision += precision_k
    sum_recall += recall_k
    sum_ndcg += ndcg_k

    # ìœ ì €ë³„ë¡œ ë””ë²„ê¹…ìš© ì¶œë ¥ í•˜ê³  ì‹¶ë‹¤ë©´ ì£¼ì„ í•´ì œ
    # print(f"user {u}: |true_pos|={len(true_pos_titles)}, hits={hits}, P@{K_EVAL}={precision_k:.3f}, R@{K_EVAL}={recall_k:.3f}, NDCG@{K_EVAL}={ndcg_k:.3f}")


# ----------------- ìµœì¢… í‰ê·  ì§€í‘œ ê³„ì‚° -----------------
if num_users_with_pos > 0:
    avg_precision = sum_precision / num_users_with_pos
    avg_recall = sum_recall / num_users_with_pos
    avg_ndcg = sum_ndcg / num_users_with_pos
    hit_rate = num_users_with_hit / num_users_with_pos
else:
    avg_precision = avg_recall = avg_ndcg = hit_rate = 0.0

print("\n=== ğŸ“ˆ Top-K Recommendation Metrics ===")
print(f"Test Samples (rows)           : {len(df_test)}")
print(f"Unique users in test          : {num_users_total}")
print(f"Users with at least 1 positive: {num_users_with_pos}")
print(f"Evaluation Top-K (K_EVAL)     : {K_EVAL}")
print("----------------------------------------------")
print(f"Hit Rate@{K_EVAL}   (user-level) : {hit_rate:.4f}")
print(f"Precision@{K_EVAL} (macro-avg)  : {avg_precision:.4f}")
print(f"Recall@{K_EVAL}    (macro-avg)  : {avg_recall:.4f}")
print(f"NDCG@{K_EVAL}      (macro-avg)  : {avg_ndcg:.4f}")
print("----------------------------------------------")
print("â€» Hit Rate@K: ì–‘ì„± ê°€ì§„ ìœ ì €ë“¤ ì¤‘ ì ì–´ë„ í•˜ë‚˜ëŠ” ë§ì¶˜ ìœ ì € ë¹„ìœ¨")
print("â€» Precision/Recall/NDCG@K: ìœ ì €ë³„ ê°’ì„ í‰ê·  ë‚¸ macro-avg ê¸°ì¤€")
